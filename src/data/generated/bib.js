const generatedBibEntries = {
    "Agarwal2021Knowledge": {
        "abstract": "Multimodal Deep Learning has garnered much interest, and transformers have triggered novel approaches, thanks to the cross-attention mechanism. Here we propose an approach to deal with two key existing challenges: the high computational resource demanded and the issue of missing modalities. We introduce for the first time the concept of knowledge distillation in transformers to use only one modality at inference time. We report a full study analyzing multiple student-teacher configurations, levels at which distillation is applied, and different methodologies. With the best configuration, we improved the state-of-the-art accuracy by 3%, we reduced the number of parameters by 2.5 times and the inference time by 22%. Such performance-computation tradeoff can be exploited in many applications and we aim at opening a new research area where the deployment of complex models with limited resources is demanded.",
        "author": "Dhruv Agarwal, Tanay Agrawal, Laura M. Ferrari, Francois Bremond",
        "doi": "10.1109/AVSS.2021.9663793",
        "journal": "2021 17th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)",
        "keywords": "knowledge distillation, multimodal, transformer",
        "title": "From Multimodal to Unimodal Attention in Transformers using Knowledge Distillation",
        "type": "article",
        "url": "https://doi.org/10.1109/AVSS.2021.9663793",
        "year": "2021"
    },
    "Agarwal2021Multimodal": {
        "abstract": "Multimodal Deep Learning has garnered much interest, and transformers have triggered novel approaches, thanks to the cross-attention mechanism. Here we propose an approach to deal with two key existing challenges: the high computational resource demanded and the issue of missing modalities. We introduce for the first time the concept of knowledge distillation in transformers to use only one modality at inference time. We report a full study analyzing multiple student-teacher configurations, levels at which distillation is applied, and different methodologies. With the best configuration, we improved the state-of-the-art accuracy by 3%, we reduced the number of parameters by 2.5 times and the inference time by 22%. Such performance-computation tradeoff can be exploited in many applications and we aim at opening a new research area where the deployment of complex models with limited resources is demanded.",
        "author": "Dhruv Agarwal, Tanay Agrawal, Laura M. Ferrari, Francois Bremond",
        "doi": "10.1109/AVSS52988.2021.9663793",
        "journal": "2021 17th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)",
        "keywords": "knowledge distillation, multimodal, transformer",
        "title": "From Multimodal to Unimodal Attention in Transformers using Knowledge Distillation",
        "type": "article",
        "url": "https://doi.org/10.1109/AVSS52988.2021.9663793",
        "year": "2021"
    },
    "Beck2016Visual": {
        "abstract": "Bibiographic data such as collections of scientific articles and citation networks have been studied extensively in information visualization and visual analytics research. Powerful systems have been built to support various types of bibliographic analysis, but they require some training and cannot be used to disseminate the insights gained. In contrast, we focused on developing a more accessible visual analytics system, called SurVis, that is ready to disseminate a carefully surveyed literature collection. The authors of a survey may use our Web-based system to structure and analyze their literature database. Later, readers of the survey can obtain an overview, quickly retrieve specific publications, and reproduce or extend the original bibliographic analysis. Our system employs a set of selectors that enable users to filter and browse the literature collection as well as to control interactive visualizations. The versatile selector concept includes selectors for textual search, filtering by keywords and meta-information, selection and clustering of similar publications, and following citation links. Agreement to the selector is represented by word-sized sparkline visualizations seamlessly integrated into the user interface. Based on an analysis of the analytical reasoning process, we derived requirements for the system. We developed the system in a formative way involving other researchers writing literature surveys. A questionnaire study with 14 visual analytics experts confirms that SurVis meets the initially formulated requirements.",
        "author": "Beck, Fabian and Koch, Sebastian and Weiskopf, Daniel",
        "doi": "10.1109/TVCG.2015.2467757",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:system, visual_analytics, sparklines, information_retrieval, clustering, literature_browser",
        "number": "01",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "Visual Analysis and Dissemination of Scientific Literature Collections with {SurVis}",
        "type": "article",
        "url": "https://doi.org/",
        "volume": "22",
        "year": "2016"
    },
    "Dai2022Vision": {
        "abstract": "The recent large-scale vision-language pre-training (VLP) of dual-stream architectures (e.g., CLIP) with a tremendous amount of image-text pair data, has shown its superiority on various multimodal alignment tasks. Despite its success, the resulting models are not capable of multimodal generative tasks due to the weak text encoder. To tackle this problem, we propose to augment the dual-stream VLP model with a textual pre-trained language model (PLM) via vision-language knowledge distillation (VLKD), enabling the capability for multimodal generation. VLKD is pretty data- and computation-efficient compared to the pre-training from scratch. Experimental results show that the resulting model has strong zero-shot performance on multimodal generation tasks, such as open-ended visual question answering and image captioning. For example, it achieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous state-of-the-art zero-shot model with 7\u00d7 fewer parameters. Furthermore, the original textual language understanding and generation ability of the PLM is maintained after VLKD, which makes our model versatile for both multimodal and unimodal tasks.",
        "author": "Wenliang Dai, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu, Pascale Fung",
        "doi": "10.48550/arXiv.2203.06386",
        "journal": "arXiv preprint arXiv:2203.06386",
        "keywords": "vision-language pre-training, knowledge distillation, multimodal generation",
        "title": "Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation",
        "type": "article",
        "url": "https://arxiv.org/10.48550/arXiv.2203.06386",
        "year": "2022"
    },
    "Li2023Interactive": {
        "abstract": "Large-scale pre-training has brought unimodal fields such as computer vision and natural language processing to a new era. Following this trend, the size of multimodal learning models constantly increases, leading to an urgent need to reduce the massive computational cost of finetuning these models for downstream tasks. In this paper, we propose an efficient and flexible multimodal fusion method, namely PMF, tailored for fusing unimodally pre-trained transformers. Specifically, we first present a modular multimodal fusion framework that exhibits high flexibility and facilitates mutual interactions among different modalities. In addition, we disentangle vanilla prompts into three types in order to learn different optimizing objectives for multimodal learning. It is also worth noting that we propose to add prompt vectors only on the deep layers of the unimodal transformers, thus significantly reducing the training memory usage. Experiment results show that our proposed method achieves comparable performance to several other multimodal finetuning methods with less than 3% trainable parameters and up to 66% saving of training memory usage.",
        "author": "Yaowei Li, Ruijie Quan, Linchao Zhu, Yi Yang",
        "doi": "10.1109/CVPR.2023.00152",
        "journal": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
        "keywords": "multimodal fusion, interactive prompting, computational efficiency",
        "title": "Efficient Multimodal Fusion via Interactive Prompting",
        "type": "article",
        "url": "https://doi.org/10.1109/CVPR.2023.00152",
        "year": "2023"
    },
    "Malihi2023Compression": {
        "abstract": "Efficient model deployment is a key focus in deep learning. This has led to the exploration of methods such as knowledge distillation and network pruning to compress models and increase their performance. In this study, we investigate the potential synergy between knowledge distillation and network pruning to achieve optimal model efficiency and improved generalization. We introduce an innovative framework for model compression that combines knowledge distillation, pruning, and fine-tuning to achieve enhanced compression while providing control over the degree of compactness. Our research is conducted on popular datasets, CIFAR-10 and CIFAR-100, employing diverse model architectures, including ResNet, DenseNet, and EfficientNet. We could calibrate the amount of compression achieved. This allows us to produce models with different degrees of compression while still being just as accurate, or even better. Notably, we demonstrate its efficacy by producing two compressed variants of ResNet 101: ResNet 50 and ResNet 18. Our results reveal intriguing findings. In most cases, the pruned and distilled student models exhibit comparable or superior accuracy to the distilled student models while utilizing significantly fewer parameters.",
        "author": "Leila Malihi, Gunther Heidemann",
        "doi": "10.3390/bdcc7030154",
        "journal": "Big Data and Cognitive Computing",
        "keywords": "knowledge distillation, model compression, pruning",
        "series": "BDCC",
        "title": "Efficient and Controllable Model Compression through Sequential Knowledge Distillation and Pruning",
        "type": "article",
        "url": "https://doi.org/10.3390/bdcc7030154",
        "volume": "7",
        "year": "2023"
    },
    "Sun2023Efficient": {
        "abstract": "With the proliferation of user-generated online videos, Multimodal Sentiment Analysis (MSA) has attracted increasing attention recently. Despite significant progress, there are still two major challenges on the way towards robust MSA: inefficiency when modeling cross-modal interactions in unaligned multimodal data and vulnerability to random modality feature missing which typically occurs in realistic settings. In this paper, we propose a generic and unified framework to address them, named Efficient Multimodal Transformer with Dual-Level Feature Restoration (EMT-DLFR). Concretely, EMT employs utterance-level representations from each modality as the global multimodal context to interact with local unimodal features and mutually promote each other. It not only avoids the quadratic scaling cost of previous local-local cross-modal interaction methods but also leads to better performance. To improve model robustness in the incomplete modality setting, on the one hand, DLFR performs low-level feature reconstruction to implicitly encourage the model to learn semantic information from incomplete data. On the other hand, it innovatively regards complete and incomplete data as two different views of one sample and utilizes siamese representation learning to explicitly attract their high-level representations. Comprehensive experiments on three popular datasets demonstrate that our method achieves superior performance in both complete and incomplete modality settings.",
        "author": "Licai Sun, Zheng Lian, Bin Liu, Jianhua Tao",
        "doi": "10.1109/TAFFC.2023.3254587",
        "journal": "IEEE Transactions on Affective Computing",
        "keywords": "multimodal sentiment analysis, feature restoration, transformer, robustness",
        "series": "TAFFC",
        "title": "Efficient Multimodal Transformer With Dual-Level Feature Restoration for Robust Multimodal Sentiment Analysis",
        "type": "article",
        "url": "https://doi.org/10.1109/TAFFC.2023.3254587",
        "volume": "15",
        "year": "2024"
    },
    "Wang2020Multimodal": {
        "abstract": "Multimodal learning aims at utilizing information from a variety of data modalities to improve the generalization performance. One common approach is to seek the common information that is shared among different modalities for learning, whereas we can also fuse the supplementary information to leverage modality-specific information. Though the supplementary information is often desired, most existing multimodal approaches can only learn from samples with complete modalities, which wastes a considerable amount of data collected. Otherwise, model-based imputation needs to be used to complete the missing values and yet may introduce undesired noise, especially when the sample size is limited. In this paper, we proposed a framework based on knowledge distillation, utilizing the supplementary information from all modalities, and avoiding imputation and noise associated with it. Specifically, we first train models on each modality independently using all the available data. Then the trained models are used as teachers to teach the student model, which is trained with the samples having complete modalities. We demonstrate the effectiveness of the proposed method in extensive empirical studies on both synthetic datasets and real-world datasets.",
        "author": "Qi Wang, Liang Zhan, Paul Thompson, Jiayu Zhou",
        "booktitle": "Proceedings of the 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)",
        "doi": "10.1145/3394486.3403234",
        "keywords": "multimodal learning, knowledge distillation, incomplete modalities",
        "title": "Multimodal Learning with Incomplete Modalities by Knowledge Distillation",
        "type": "inproceedings",
        "url": "https://doi.org/10.1145/3394486.3403234",
        "year": "2020"
    },
    "Wang2023VideoAdviser": {
        "abstract": "Multimodal transfer learning aims to transform pretrained representations of diverse modalities into a common domain space for effective multimodal fusion. However, conventional systems are typically built on the assumption that all modalities exist, and the lack of modalities always leads to poor inference performance. Furthermore, extracting pretrained embeddings for all modalities is computationally inefficient for inference. In this work, to achieve high efficiency-performance multimodal transfer learning, we propose VideoAdviser, a video knowledge distillation method to transfer multimodal knowledge of video-enhanced prompts from a multimodal fundamental model (teacher) to a specific modal fundamental model (student). With an intuition that the best learning performance comes with professional advisers and smart students, we use a CLIP-based teacher model to provide expressive multimodal knowledge supervision signals to a RoBERTa-based student model via optimizing a step-distillation objective loss. The student (requiring only the text modality as input) achieves an MAE score improvement of up to 12.3% for MOSI and MOSEI. Our method further enhances the state-of-the-art method by 3.4% mAP score for VEGAS without additional computations for inference. These results suggest the strengths of our method for achieving high efficiency-performance multimodal transfer learning.",
        "author": "Yanan Wang, Donghuo Zeng, Shinya Wada, Satoshi Kurihara",
        "doi": "10.1109/ACCESS.2023.3280187",
        "journal": "IEEE Access",
        "keywords": "multimodal transfer learning, knowledge distillation, video",
        "series": "IEEE Access",
        "title": "VideoAdviser: Video Knowledge Distillation for Multimodal Transfer Learning",
        "type": "article",
        "url": "https://doi.org/10.1109/ACCESS.2023.3280187",
        "volume": "11",
        "year": "2023"
    },
    "Xue2023Dynamic": {
        "abstract": "Deep multimodal learning has achieved great progress in recent years. However, current fusion approaches are static in nature, i.e., they process and fuse multimodal inputs with identical computation, without accounting for diverse computational demands of different multimodal data. In this work, we propose dynamic multimodal fusion (DynMM), a new approach that adaptively fuses multimodal data and generates data-dependent forward paths during inference. To this end, we propose a gating function to provide modality-level or fusion-level decisions on-the-fly based on multimodal features and a resource-aware loss function that encourages computational efficiency. Results on various multimodal tasks demonstrate the efficiency and wide applicability of our approach. For instance, DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation) when compared with static fusion approaches. We believe our approach opens a new direction towards dynamic multimodal network design, with applications to a wide range of multimodal tasks.",
        "author": "Zihui Xue, Radu Marculescu",
        "doi": "10.1109/CVPRW.2023.00145",
        "journal": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops",
        "keywords": "dynamic multimodal fusion, computational efficiency, sentiment analysis, semantic segmentation",
        "title": "Dynamic Multimodal Fusion for Robust Multimodal Sentiment Analysis",
        "type": "article",
        "url": "https://doi.org/10.1109/CVPRW.2023.00145",
        "year": "2023"
    }
};